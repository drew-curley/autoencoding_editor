import os
import re
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
import string

# Set directory
DATA_DIR = '/home/curleyd/GitHub/New'

# Text preprocessing
def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    text = text.strip()  # Strip whitespace
    return text

# Extract chapter and verse numbers from line
def extract_chapter_verse(line):
    match = re.search(r'(\d+):(\d+)', line)
    if match:
        return match.group(1), match.group(2)
    return None, None

# Load text files with metadata
def load_text_files(directory):
    verses = []
    for filename in sorted(os.listdir(directory)):
        if any(filename.startswith(str(i)) for i in range(41, 68)) and filename.endswith('.usfm'):
            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:
                lines = file.readlines()
                for line in lines:
                    if line.strip():
                        chapter, verse = extract_chapter_verse(line)
                        text = preprocess_text(line)
                        verses.append({
                            'file_name': filename,
                            'chapter': chapter,
                            'verse_number': verse,
                            'verse_text': text
                        })
    return verses

verses = load_text_files(DATA_DIR)
texts = [v['verse_text'] for v in verses]
print(f"Loaded {len(texts)} lines from text files 41 to 67.")

# Tokenize the text
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)

sequences = tokenizer.texts_to_sequences(texts)
vocab_size = len(tokenizer.word_index) + 1

# Pad sequences
max_length = 50
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')

# Prepare input for autoencoder (categorical labels)
x_train = np.array(padded_sequences)
y_train = keras.utils.to_categorical(x_train, num_classes=vocab_size)

embedding_dim = 128
latent_dim = 64

# Build autoencoder
def build_autoencoder():
    # Encoder
    encoder_input = layers.Input(shape=(max_length,))
    x = layers.Embedding(vocab_size, embedding_dim, input_length=max_length)(encoder_input)
    x = layers.LSTM(latent_dim, return_sequences=False)(x)
    latent_vector = layers.Dense(latent_dim, activation='relu')(x)

    # Decoder
    x = layers.RepeatVector(max_length)(latent_vector)
    x = layers.LSTM(latent_dim, return_sequences=True)(x)
    decoder_output = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(x)

    autoencoder = keras.Model(encoder_input, decoder_output)
    encoder = keras.Model(encoder_input, latent_vector)
    return autoencoder, encoder

autoencoder, encoder = build_autoencoder()
autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')
autoencoder.summary()

# **EarlyStopping callback**
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the autoencoder
autoencoder.fit(x_train, y_train, 
                epochs=100, 
                batch_size=32, 
                validation_split=0.2, 
                callbacks=[early_stopping])

# Improved reconstruction error calculation
def calculate_reconstruction_error(sentence):
    sequence = tokenizer.texts_to_sequences([sentence])
    sequence = pad_sequences(sequence, maxlen=max_length, padding='post')
    sequence = np.clip(sequence, 0, vocab_size - 1)

    predicted_seq = autoencoder.predict(sequence)
    predicted_seq = np.clip(predicted_seq, 1e-10, 1.0)

    sequence_flat = sequence.flatten()
    predicted_seq_flat = predicted_seq.reshape(-1, predicted_seq.shape[-1])

    error = -np.sum(np.log(predicted_seq_flat[np.arange(sequence_flat.shape[0]), sequence_flat]))
    return error

# Calculate reconstruction error for all verses
errors = [calculate_reconstruction_error(v['verse_text']) for v in verses]

# Select top 10% of verses with the highest reconstruction error
num_verses_to_select = int(len(verses) * 0.10)
verses_with_errors = pd.DataFrame(verses)
verses_with_errors['error'] = errors
top_verses = verses_with_errors.nlargest(num_verses_to_select, 'error')

# Output the list of verses
print("Top 10% of verses with highest reconstruction error:")
print(top_verses[['file_name', 'chapter', 'verse_number', 'verse_text', 'error']].to_string(index=False))

# Export results to CSV
top_verses[['file_name', 'chapter', 'verse_number', 'verse_text', 'error']].to_csv('verses_with_high_error.csv', index=False)
print("Results saved to verses_with_high_error.csv")
