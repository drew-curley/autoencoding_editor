import os
import logging
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch(utils.data import Dataset, DataLoader, random_split
from transformers import BertTokenizer, MBartForConditionalGeneration, MBart50TokenizerFast
from transformers import Trainer, TrainingArguments
from datasets import Dataset
from tqdm import tqdm

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Configuration
config = {
    'bitext_csv': os.path.expanduser("~/Desktop/bitexts.csv"),
    'output_csv': os.path.expanduser("~/Desktop/anomalous_with_alternatives.csv"),
    'zrl_col': 4,  # Zero-resource language (column 5, 0-based index)
    'english_col': 5,  # English (column 6)
    'zrl_lang_code': 'de_DE',  # Placeholder for ZRL; adjust if known
    'max_length': 50,
    'embedding_dim': 128,
    'latent_dim': 128,
    'mae_epochs': 10,
    'batch_size': 32,
    'val_split': 0.2,
    'learning_rate': 1e-4,
    'mask_prob': 0.3,
    'error_threshold': 0.1,
    'early_stopping_patience': 5,
    'translation_epochs': 3,
    'translation_batch_size': 16,
    'device': torch.device("cuda" if torch.cuda.is_available() else "cpu"),
}

# Load bitexts.csv
df = pd.read_csv(config['bitext_csv'], header=None)
df['id'] = df.iloc[:, 0].astype(str)

# Initialize BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
config['vocab_size'] = tokenizer.vocab_size

# Dataset for MAE
class MaskedTextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length, mask_prob):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.texts = texts
        self.mask_prob = mask_prob
        self.mask_token_id = tokenizer.mask_token_id

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text, padding="max_length", truncation=True, max_length=self.max_length, return_tensors="pt"
        )
        input_ids = encoding["input_ids"].squeeze(0)
        masked_input = input_ids.clone()
        mask = torch.rand(input_ids.shape) < self.mask_prob
        masked_input[mask] = self.mask_token_id
        return masked_input, input_ids

# Masked Autoencoder model
class MaskedAutoencoder(nn.Module):
    def __init__(self, vocab_size, max_length, embedding_dim, latent_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dim_feedforward=latent_dim)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)
        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=4, dim_feedforward=latent_dim)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)
        self.output_layer = nn.Linear(embedding_dim, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x).transpose(0, 1)
        encoded = self.encoder(embedded)
        decoded = self.decoder(encoded, encoded)
        logits = self.output_layer(decoded).transpose(0, 1)
        return logits

# Compute reconstruction error
def compute_error(model, tokenizer, text, max_length, device):
    encoding = self.tokenizer(
        text, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt"
    )
    input_ids = encoding["input_ids"].to(device)
    model.eval()
    with torch.no_grad():
        logits = model(input_ids)
        probs = F.softmax(logits, dim=-1)
        predicted_ids = torch.argmax(probs, dim=-1)
    error = torch.mean((input_ids != predicted_ids).float()).item()
    return error

# Train MAE
def train_mae(model, train_loader, val_loader, criterion, optimizer, config):
    best_val_loss = float('inf')
    patience_counter = 0
    for epoch in range(config['mae_epochs']):
        model.train()
        train_loss = 0
        for masked_input, target in tqdm(train_loader, desc=f"MAE Epoch {epoch+1}/{config['mae_epochs']}"):
            masked_input, target = masked_input.to(config['device']), target.to(config['device'])
            logits = model(masked_input)
            loss = criterion(logits.reshape(-1, config['vocab_size']), target.reshape(-1))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss_avg = train_loss / len(train_loader)
        logging.info(f"MAE Epoch {epoch+1} - Train Loss: {train_loss_avg:.6f}")

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for masked_input, target in val_loader:
                masked_input, target = masked_input.to(config['device']), target.to(config['device'])
                logits = model(masked_input)
                loss = criterion(logits.reshape(-1, config['vocab_size']), target.reshape(-1))
                val_loss += loss.item()
        val_loss_avg = val_loss / len(val_loader)
        logging.info(f"MAE Epoch {epoch+1} - Validation Loss: {val_loss_avg:.6f}")

        if val_loss_avg < best_val_loss:
            best_val_loss = val_loss_avg
            patience_counter = 0
            torch.save(model.state_dict(), "best_mae.pth")
        else:
            patience_counter += 1
            if patience_counter >= config['early_stopping_patience']:
                logging.info("Early stopping triggered for MAE.")
                break

# Identify anomalous sentences
def get_anomalous_sentences(model, tokenizer, texts, ids, max_length, device, threshold):
    anomalous = []
    model.eval()
    for idx, text in tqdm(enumerate(texts), total=len(texts), desc="Identifying anomalies"):
        error = compute_error(model, tokenizer, text, max_length, device)
        if error > threshold:
            anomalous.append((ids[idx], text, error))
    return sorted(anomalous, key=lambda x: x[2], reverse=True)  # Sort by error descending

# Fine-tune mBART for English to ZRL translation
def fine_tune_mbart(df, config):
    model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
    tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
    tokenizer.tgt_lang = config['zrl_lang_code']

    # Prepare dataset
    parallel_data = [
        {"src": row[config['english_col']], "tgt": row[config['zrl_col']]}
        for _, row in df.iterrows()
    ]
    dataset = Dataset.from_list(parallel_data)

    def tokenize_function(examples):
        src = [ex["src"] for ex in examples]
        tgt = [ex["tgt"] for ex in examples]
        model_inputs = tokenizer(src, text_target=tgt, padding="max_length", 
                                 truncation=True, max_length=config['max_length'])
        return model_inputs

    tokenized_dataset = dataset.map(tokenize_function, batched=True)

    # Split dataset
    train_size = int((1 - config['val_split']) * len(tokenized_dataset))
    train_dataset = tokenized_dataset.select(range(train_size))
    eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))

    # Training arguments
    training_args = TrainingArguments(
        output_dir="./mbart_results",
        evaluation_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=config['translation_batch_size'],
        per_device_eval_batch_size=config['translation_batch_size'],
        num_train_epochs=config['translation_epochs'],
        weight_decay=0.01,
        save_strategy="epoch",
        load_best_model_at_end=True,
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    # Fine-tune
    trainer.train()

    # Save model
    model.save_pretrained("fine_tuned_mbart")
    tokenizer.save_pretrained("fine_tuned_mbart")
    return model, tokenizer

# Generate alternative rendering
def generate_alternative(english_text, model, tokenizer, config):
    model.eval()
    inputs = tokenizer(
        english_text, return_tensors="pt", padding=True, 
        truncation=True, max_length=config['max_length']
    ).to(config['device'])
    translated_ids = model.generate(
        **inputs, 
        forced_bos_token_id=tokenizer.lang_code_to_id[config['zrl_lang_code']],
        max_length=config['max_length']
    )
    alternative_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)
    return alternative_text

# Main function
def main():
    # Extract ZRL texts for MAE training
    zrl_texts = df.iloc[:, config['zrl_col']].astype(str).tolist()
    ids = df['id'].tolist()

    # Train MAE
    dataset = MaskedTextDataset(zrl_texts, tokenizer, config['max_length'], config['mask_prob'])
    train_size = int((1 - config['val_split']) * len(dataset))
    val_size = len(dataset) - train_size
    train_set, val_set = random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_set, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_set, batch_size=config['batch_size'])

    mae_model = MaskedAutoencoder(
        config['vocab_size'], config['max_length'], config['embedding_dim'], config['latent_dim']
    ).to(config['device'])
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    optimizer = optim.Adam(mae_model.parameters(), lr=config['learning_rate'])
    train_mae(mae_model, train_loader, val_loader, criterion, optimizer, config)
    mae_model.load_state_dict(torch.load("best_mae.pth"))  # Load best model

    # Identify anomalous sentences
    anomalous_sentences = get_anomalous_sentences(
        mae_model, tokenizer, zrl_texts, ids, config['max_length'], config['device'], config['error_threshold']
    )
    logging.info(f"Found {len(anomalous_sentences)} anomalous sentences.")

    # Fine-tune mBART
    mbart_model, mbart_tokenizer = fine_tune_mbart(df, config)
    mbart_model.to(config['device'])

    # Generate alternative renderings
    results = []
    for sent_id, original_text, error in tqdm(anomalous_sentences, desc="Generating alternatives"):
        try:
            # Find the corresponding English text
            english_text = df[df['id'] == sent_id].iloc[0, config['english_col']]
            # Generate alternative ZRL text
            alternative_text = generate_alternative(english_text, mbart_model, mbart_tokenizer, config)
            # Compute error for alternative
            alt_error = compute_error(mae_model, tokenizer, alternative_text, config['max_length'], config['device'])
            results.append({
                'id': sent_id,
                'text_original': original_text,
                'text_alternative': alternative_text,
                'error_original': error,
                'error_alternative': alt_error
            })
        except Exception as e:
            logging.warning(f"Failed to process ID {sent_id}: {e}")
            continue

    # Save results
    results_df = pd.DataFrame(results)
    results_df.to_csv(config['output_csv'], index=False)
    logging.info(f"Saved results to {config['output_csv']}")

if __name__ == "__main__":
    main()
