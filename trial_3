import os
import re
import string
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

###########################
# File paths
###########################
INPUT_CSV = '/home/curleyd/GitHub/New/output.csv'
OUTPUT_CSV = '/home/curleyd/GitHub/New/verses_with_high_error.csv'

###########################
# Hyperparameters
###########################
NUM_WORDS = 10000       # Only keep top 10k words
MAX_LENGTH = 50         # Max tokens per line
EMBEDDING_DIM = 128
LATENT_DIM = 64
EPOCHS = 100
BATCH_SIZE = 32
VAL_SPLIT = 0.2

###########################
# Text preprocessing
###########################
def preprocess_for_model(text):
    """
    Lowercase, remove punctuation, strip extra spaces. 
    This is for feeding into the model (not for final output).
    """
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.strip()
    return text

###########################
# Load CSV with single column
###########################
def load_csv_with_id_and_text(csv_path):
    """
    Loads the CSV (one column, each row = '8-digit ID' + ' text').
    Returns a DataFrame with:
      - 'raw': the entire line
      - 'id': the 8-digit ID
      - 'original_text': the text after the ID (exactly as in the file, minus leading space)
      - 'clean_text': text preprocessed for the model
    """
    # Read CSV into a single column named 'raw'
    df = pd.read_csv(csv_path, header=None, names=['raw'])

    # Extract 8-digit ID from the first 8 chars
    df['id'] = df['raw'].str[:8]

    # Extract the text (skip the first 8 chars, then strip left space)
    df['original_text'] = df['raw'].str[8:].str.lstrip()

    # Create a 'clean_text' column for the model
    df['clean_text'] = df['original_text'].apply(preprocess_for_model)

    return df

###########################
# Build Autoencoder
###########################
def build_autoencoder(vocab_size, max_length, embedding_dim, latent_dim):
    # Encoder
    encoder_input = layers.Input(shape=(max_length,))
    x = layers.Embedding(vocab_size, embedding_dim, input_length=max_length)(encoder_input)
    x = layers.LSTM(latent_dim, return_sequences=False)(x)
    latent_vector = layers.Dense(latent_dim, activation='relu')(x)

    # Decoder
    x = layers.RepeatVector(max_length)(latent_vector)
    x = layers.LSTM(latent_dim, return_sequences=True)(x)
    decoder_output = layers.TimeDistributed(
        layers.Dense(vocab_size, activation='softmax')
    )(x)

    autoencoder = keras.Model(encoder_input, decoder_output)
    encoder = keras.Model(encoder_input, latent_vector)
    
    return autoencoder, encoder

###########################
# Reconstruction Error
###########################
def calculate_reconstruction_error(text, tokenizer, autoencoder, max_length, vocab_size):
    """
    Negative log-likelihood of the correct words given the autoencoder's predictions.
    """
    # Convert text to sequence
    sequence = tokenizer.texts_to_sequences([text])
    sequence = pad_sequences(sequence, maxlen=max_length, padding='post')
    sequence = np.clip(sequence, 0, vocab_size - 1)

    # Predict
    predicted_seq = autoencoder.predict(sequence)
    predicted_seq = np.clip(predicted_seq, 1e-10, 1.0)  # avoid log(0)

    # Flatten
    sequence_flat = sequence.flatten()
    predicted_seq_flat = predicted_seq.reshape(-1, predicted_seq.shape[-1])

    # negative log-likelihood for each token
    error = -np.sum(
        np.log(predicted_seq_flat[
            np.arange(sequence_flat.shape[0]),
            sequence_flat
        ])
    )
    return error

###########################
# Main
###########################
def main():
    # 1) Load data
    df = load_csv_with_id_and_text(INPUT_CSV)
    print(f'Loaded {len(df)} rows from {INPUT_CSV}.')

    # 2) Tokenize
    texts_for_tokenizer = df['clean_text'].tolist()
    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token="<OOV>")
    tokenizer.fit_on_texts(texts_for_tokenizer)

    # Convert each cleaned text to sequence
    sequences = tokenizer.texts_to_sequences(texts_for_tokenizer)
    vocab_size = len(tokenizer.word_index) + 1  # dynamic vocab size

    # 3) Pad sequences
    padded_sequences = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post')
    x_train = np.array(padded_sequences)

    # We want one-hot labels for autoencoder
    y_train = keras.utils.to_categorical(x_train, num_classes=vocab_size)

    # 4) Build autoencoder
    autoencoder, encoder = build_autoencoder(vocab_size, MAX_LENGTH, EMBEDDING_DIM, LATENT_DIM)
    autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')
    autoencoder.summary()

    # 5) Early stopping
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=3, restore_best_weights=True
    )

    # 6) Train
    autoencoder.fit(
        x_train,
        y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_split=VAL_SPLIT,
        callbacks=[early_stopping]
    )

    # 7) Calculate errors
    errors = []
    for text in df['clean_text']:
        err = calculate_reconstruction_error(text, tokenizer, autoencoder, MAX_LENGTH, vocab_size)
        errors.append(err)
    df['error'] = errors

    # 8) Select top 10%
    num_to_select = int(len(df) * 0.10)
    top_df = df.nlargest(num_to_select, 'error')

    # 9) Reattach the original ID + original text
    #    EXACT format: "41001001 Iwu jiwu mukanda..."
    top_df['final_output'] = top_df['id'] + ' ' + top_df['original_text']

    # Show results in console
    print(f'\nTop {num_to_select} lines (10%) with highest reconstruction error:\n')
    print(top_df[['final_output', 'error']].to_string(index=False))

    # 10) Save to CSV
    top_df[['final_output', 'error']].to_csv(OUTPUT_CSV, index=False)
    print(f'\nResults saved to {OUTPUT_CSV}')

if __name__ == '__main__':
    main()
